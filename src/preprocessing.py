# [GHI CH√ö D√ÄNH CHO NH√ìM PH√ÅT TRI·ªÇN]
# -----------------------------------
# T·ªáp tin n√†y cung c·∫•p khung s∆∞·ªùn (template) c∆° b·∫£n cho l·ªõp x·ª≠ l√Ω d·ªØ li·ªáu.
# C√°c th√†nh vi√™n c√≥ quy·ªÅn ch·ªânh s·ª≠a, t·ªëi ∆∞u h√≥a logic b√™n trong c√°c h√†m
# ƒë·ªÉ ph√π h·ª£p v·ªõi y√™u c·∫ßu th·ª±c t·∫ø c·ªßa d·ª± √°n.
# Khuy·∫øn ngh·ªã gi·ªØ nguy√™n t√™n L·ªõp v√† c√°c ph∆∞∆°ng th·ª©c ch√≠nh (process, clean_text)
# ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh t∆∞∆°ng th√≠ch khi t√≠ch h·ª£p h·ªá th·ªëng.
# Ch·ª©c nƒÉng: Class x·ª≠ l√Ω d·ªØ li·ªáu chu·∫©n (Stopwords .txt + Teencode .csv)

import pandas as pd
import re
import unicodedata
import emoji  # C·∫ßn pip install emoji
from pyvi import ViTokenizer 
import os
from sklearn.model_selection import train_test_split
from tqdm import tqdm

class DataPreprocessor:
    def __init__(self, mode='baseline', stopwords_path=None, teencode_path=None):
        """
        Kh·ªüi t·∫°o b·ªô x·ª≠ l√Ω d·ªØ li·ªáu.
        Tham s·ªë:
            stopwords_path: N·∫øu None -> Ch·∫ø ƒë·ªô Deep Learning (Gi·ªØ stopwords).
                        N·∫øu c√≥ path -> Ch·∫ø ƒë·ªô Statistical (X√≥a stopwords).
        """
        self.stopwords = set()
        self.teencode_dict = {} # Kh·ªüi t·∫°o r·ªóng, s·∫Ω load t·ª´ CSV

        # Load stopwords t·ª´ file .txt
        if stopwords_path and os.path.exists(stopwords_path):
            try:
                with open(stopwords_path, 'r', encoding='utf-8') as f:
                    # splitlines() t·ª± ƒë·ªông c·∫Øt d√≤ng, strip() ƒë·ªÉ x√≥a kho·∫£ng tr·∫Øng th·ª´a ƒë·∫ßu ƒëu√¥i
                    self.stopwords = set(line.strip() for line in f if line.strip())
                print(f"[Statistical Mode] ƒê√£ load {len(self.stopwords)} stopwords.")
            except Exception as e:
                print(f" L·ªói load stopwords: {e}")
        else:
            print(f"[Deep Learning Mode] Kh√¥ng d√πng Stopwords (Gi·ªØ nguy√™n vƒÉn b·∫£n).")

        # Load teencode t·ª´ file .csv
        if teencode_path and os.path.exists(teencode_path):
            try:
                df = pd.read_csv(teencode_path)
                # Ki·ªÉm tra xem file c√≥ ƒë√∫ng 2 c·ªôt c·∫ßn thi·∫øt kh√¥ng
                if 'Word' in df.columns and 'Meaning' in df.columns:
                    # Chuy·ªÉn th√†nh Dictionary {Word: Meaning}
                    # √©p ki·ªÉu str ƒë·ªÉ tr√°nh l·ªói n·∫øu file csv c√≥ s·ªë
                    self.teencode_dict = dict(zip(df['Word'].astype(str), df['Meaning'].astype(str)))
                    print(f"ƒê√£ load {len(self.teencode_dict)} teencode t·ª´ file .csv")
                else:
                    print("File CSV thi·∫øu c·ªôt 'Word' ho·∫∑c 'Meaning'")
            except Exception as e:
                print(f"L·ªói load teencode CSV: {e}")
        else:
            print(f"Kh√¥ng t√¨m th·∫•y file teencode t·∫°i: {teencode_path}")

        # Compile Regex
        if self.teencode_dict:
            # S·∫Øp x·∫øp t·ª´ d√†i tr∆∞·ªõc ng·∫Øn sau ƒë·ªÉ replace ƒë√∫ng (vd: 'ko' tr∆∞·ªõc 'k')
            sorted_keys = sorted(self.teencode_dict.keys(), key=len, reverse=True)
            self.teencode_pattern = re.compile(r'\b(' + '|'.join(re.escape(k) for k in sorted_keys) + r')\b')
        else:
            self.teencode_pattern = None

    def clean_text(self, text):
        """B∆∞·ªõc 1: Basic Cleaning & Formatting"""
        if not isinstance(text, str): return ""
        
        # 1. Chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng
        text = text.lower()
        
        # 2. X√≥a HTML tags
        text = re.sub(r'<[^>]*>', ' ', text)
        
        # 3. X√≥a URL/Link
        text = re.sub(r'http\S+|www\.\S+', '', text)
        
        # [C·∫¨P NH·∫¨T]: X√≥a Mentions (@user) theo regex chu·∫©n ASCII ƒë·ªÉ tr√°nh d√≠nh ch·ªØ Vi·ªát
        # Regex c≈©: r'@\w+' -> Regex m·ªõi: r'@[a-zA-Z0-9_.]+'
        text = re.sub(r'@[a-zA-Z0-9_.]+', '', text)
        
        # [B·ªî SUNG]: X√≥a Hashtag (#trend)
        text = re.sub(r'#\S+', '', text)
        
        # [C·∫¨P NH·∫¨T]: X√≥a k√Ω t·ª± xu·ªëng d√≤ng, tab th√†nh kho·∫£ng tr·∫Øng
        text = re.sub(r'[\n\t]', ' ', text)
        
        # X√≥a kho·∫£ng tr·∫Øng th·ª´a
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text

    def handle_emoji(self, text):
        """
        [C·∫¨P NH·∫¨T] B∆∞·ªõc 4: Emoji Handling (D√πng th∆∞ vi·ªán ti·∫øng Anh)
        """
        # demojize: chuy·ªÉn üò≠ -> :loudly_crying_face:
        # delimiters=(' ', ' '): th√™m kho·∫£ng tr·∫Øng 2 b√™n ->  loudly_crying_face 
        text = emoji.demojize(text, delimiters=(' ', ' '))
        
        # Thay th·∫ø d·∫•u : v√† _ th√†nh kho·∫£ng tr·∫Øng ƒë·ªÉ t√°ch h·∫≥n ra th√†nh t·ª´ ƒë∆°n
        # VD: :loudly_crying_face: -> loudly crying face
        text = text.replace(':', '').replace('_', ' ')
        
        return text

    def normalize(self, text):
        """
        B∆∞·ªõc 2, 3, 5, 6: Chu·∫©n h√≥a Unicode, Spam char, Teencode, D·∫•u c√¢u
        """
        # B∆∞·ªõc 2: Chu·∫©n h√≥a Unicode (NFC)
        text = unicodedata.normalize('NFC', text)
        
        # B∆∞·ªõc 4: X·ª≠ l√Ω Emoji b·∫±ng th∆∞ vi·ªán
        text = self.handle_emoji(text)
        
        # X·ª≠ l√≠ ri√™ng cho t·ª´ "kg": 'kilogram' ho·∫∑c l√† 'kh√¥ng'
        # Case A: kg l√† KILOGRAM (n·∫øu ƒë·ª©ng sau con s·ªë). VD: 5kg -> 5 kilogram
        text = re.sub(r'(\d+)\s*kg\b', r'\1 kilogram', text)
        # Case B: kg l√† KH√îNG (c√°c tr∆∞·ªùng h·ª£p c√≤n l·∫°i). VD: "nh√¨n kg ƒë·∫πp" -> "nh√¨n kh√¥ng ƒë·∫πp
        text = re.sub(r'\bkg\b', 'kh√¥ng', text)

        # B∆∞·ªõc 5: Map Teencode (T·ª´ file CSV ƒë√£ load)
        if self.teencode_pattern:
            text = self.teencode_pattern.sub(lambda x: self.teencode_dict[x.group()], text)
        
        #B∆∞·ªõc 3: Spam Character Handling
        # R√∫t g·ªçn k√Ω t·ª± l·∫∑p > 2 l·∫ßn v·ªÅ 1 k√Ω t·ª± g·ªëc (VD: ƒë·∫πpppp -> ƒë·∫πp)
        text = re.sub(r'(.)\1{2,}', r'\1', text)
        
        # B∆∞·ªõc 6: Punctuation Filtering
        # 1. Chu·∫©n h√≥a d·∫•u ba ch·∫•m: ... ho·∫∑c .... -> v·ªÅ chu·∫©n '...'
        text = re.sub(r'\.{3,}', ' ... ', text)
        
        # X√≥a d·∫•u c√¢u nhi·ªÖu:  , - * ~ ( ) 
        # Gi·ªØ l·∫°i: ! ? ...
        text = re.sub(r'[,\-*~()"]', ' ', text)

        # X√≥a d·∫•u ch·∫•m ƒë∆°n (.) nh∆∞ng kh√¥ng ·∫£nh h∆∞·ªüng d·∫•u ba ch·∫•m (...)
        text = re.sub(r'(?<!\.)\.(?!\.)', ' ', text)

        # 2. T√°ch d·∫•u c√¢u (Gi·ªØ l·∫°i ! ? ƒë·ªÉ model h·ªçc c·∫£m x√∫c)
        # VD: "qu√°!" -> "qu√° !"
        text = re.sub(r'([!?]+)', r' \1 ', text)
        
        # X√≥a kho·∫£ng tr·∫Øng th·ª´a sinh ra
        return re.sub(r'\s+', ' ', text).strip()

    def remove_stopwords(self, text):
        """[B·ªî SUNG] B∆∞·ªõc 8: Stopwords Removal"""
        if not self.stopwords:
            return text
        
        words = text.split()
        # Gi·ªØ l·∫°i t·ª´ kh√¥ng n·∫±m trong stopwords
        words = [w for w in words if w not in self.stopwords]
        return ' '.join(words)


    def process(self, text, target_model='statistical'):
        """Main Pipeline (Nhi·ªám v·ª• 1)"""
        text = self.clean_text(text)  # B∆∞·ªõc 1
        text = self.normalize(text)   # B∆∞·ªõc 2, 3, 4, 5, 6
        
        # B∆∞·ªõc 7: T√°ch t·ª´ (b·∫Øt bu·ªôc cho c·∫£ 2 mode)
        text = ViTokenizer.tokenize(text)
        
        # B∆∞·ªõc 8: Ph√¢n nh√°nh x·ª≠ l√Ω Stopwords
        if target_model == 'statistical':
            # Mode Statistical: Ch·∫°y Full 11 b∆∞·ªõc (X√≥a Stopwords)
            text = self.remove_stopwords(text)

        # Mode Deep Learning: Kh√¥ng l√†m g√¨ th√™m (Gi·ªØ nguy√™n text ƒë√£ t√°ch t·ª´)
        # V√¨ PhoBERT c·∫ßn ng·ªØ c·∫£nh ƒë·∫ßy ƒë·ªß c·ªßa c√¢u.

        return text

   
if __name__ == "__main__":
    # 1. Kh·ªüi t·∫°o
    preprocessor = DataPreprocessor(
        stopwords_path='../data/dictionaries/vietnamese_stopwords.txt',
        teencode_path='../data/dictionaries/teencode.csv'
    )
    
    # 2. ƒê·ªçc d·ªØ li·ªáu th√¥
    input_file = '../data/processed/dummy_data.csv' 
    if os.path.exists(input_file):
        df = pd.read_csv(input_file)
        df.rename(columns={'comment_text': 'text', 'comment_id': 'id'}, inplace=True)
        
        # 3. Ch·∫°y 2 l·∫ßn Pipeline cho 2 Mode
        modes = ['statistical', 'deep_learning']
        for mode in modes:
            print(f"\n ƒêang x·ª≠ l√Ω cho ch·∫ø ƒë·ªô: {mode}")
            temp_df = df.copy()
            
            # Ti·ªÅn x·ª≠ l√Ω text theo mode
            tqdm.pandas()
            temp_df['text'] = temp_df['text'].progress_apply(lambda x: preprocessor.process(x, target_model=mode))
            
            # Map nh√£n t·ª´ ch·ªØ sang s·ªë
            label_map = {'Kh√¥ng x√∫c ph·∫°m': 0, 'M·ªâa mai': 1, 'X√∫c ph·∫°m': 2}
            temp_df['label'] = temp_df['label'].map(label_map)
        
            # X√≥a c√°c d√≤ng c√≥ nh√£n (label) b·ªã tr·ªëng
            temp_df = temp_df.dropna(subset=['label'])
        
            # X√≥a c√°c d√≤ng c√≥ vƒÉn b·∫£n b·ªã tr·ªëng sau khi x·ª≠ l√Ω (v√≠ d·ª•: comment ch·ªâ c√≥ emoji b·ªã x√≥a h·∫øt)
            temp_df = temp_df[temp_df['text'].str.strip() != '']
        
            # ƒê·∫£m b·∫£o label l√† ki·ªÉu s·ªë nguy√™n (Integer)
            temp_df['label'] = temp_df['label'].astype(int)
        
            # B∆∞·ªõc 9: Deduplication (L·ªçc tr√πng)
            temp_df = temp_df.drop_duplicates(subset=['text'], keep='first')
            
            # B∆∞·ªõc 11: Data Splitting (70-15-15)
            # Split 1: T√°ch Test (15%)
            train_val, test = train_test_split(
                temp_df, test_size=0.15, stratify=temp_df['label'], random_state=42
            )
            # Split 2: T√°ch Train v√† Val (T·ª∑ l·ªá 0.15/0.85 approx 0.1765)
            train, val = train_test_split(
                train_val, test_size=0.1765, stratify=train_val['label'], random_state=42
            )
            
            # X√°c ƒë·ªãnh c√°c c·ªôt c·∫ßn gi·ªØ l·∫°i
            output_cols = ['id', 'text', 'label']

            # Xu·∫•t file (Nhi·ªám v·ª• 2 - ƒê·ªß 6 file)
            suffix = 'stat' if mode == 'statistical' else 'dl'
            processed_dir = '../data/processed'
            os.makedirs(processed_dir, exist_ok=True)
            
            train[output_cols].to_csv(f'{processed_dir}/train_{suffix}.csv', index=False)
            val[output_cols].to_csv(f'{processed_dir}/val_{suffix}.csv', index=False)
            test[output_cols].to_csv(f'{processed_dir}/test_{suffix}.csv', index=False)
            
        print("\nHo√†n th√†nh xu·∫•t 6 file output t·∫°i data/processed/!")
    else:
        print(f"L·ªói: Kh√¥ng t√¨m th·∫•y file ƒë·∫ßu v√†o t·∫°i {input_file}")